# testmodules
- 動作確認用のテストモジュール置き場。
## reduction
- metal には allreduce がないので、スレッド間の情報を足しこむアルゴリズムを実装する必要がある。
- またスレッド間の書き込み競合を防ぎながら足しこむ atomic 演算は、metal では int のみ対応しているらしい。
- ということで、reduction 演算を実装する必要がある。
- まずはGPTに出力してもらったものを真似してC言語でベタ書きして動作確認。
## allreduce
- reduction 演算の metal 並列化。
- arrsize*スレッド数 のメモリを確保しておき、後から足していく実装とした。
- ここで同時実行可能なスレッド数を粒子計算モジュールと同じにすると、arrsize*粒子数ぶんのメモリを確保する必要が出る。
- それは到底無理なので、粒子のループを分割してチャンクごとに足していく実装とした。
- このモジュールでは、スレッドグループ内の reduction 演算で shared memory を使った実装としている。
- 和がうまく計算できず色々調べていたところ、1スレッドグループあたりに定義できる共有メモリサイズを超えていたことが原因だった。
- ```Max threadgroup memory: 32768 bytes``` となり、共有メモリに定義できる float 配列のサイズは 128*64 が上限らしい。
- 共有メモリの方がおそらく動作は早いので、マシンの性能が許せばこちらを使いたい。
## allreduce_grobal
- グローバルメモリを使って allreduce する。
- arrsize*chunksize のグローバルメモリに中間の計算結果を書き込んでおき、arrsize*スレッドグループ数 のグローバルメモリに結果を出力する。
- 最終的な結果は、スレッドグループ数のループ計算を CPU 側で実施して取得する。
## poisson
- amgcl を使って poisson 方程式を解く。
- 係数行列 A を渡してソルバを生成して、初期の解 x と右辺ベクトル b を食わせるとxに解が格納されて出てくる。
- こういう仕様なので、（境界条件が時間変化しない前提であれば）ソルバを一度生成すれば使い回すことができ、PIC の時間更新に組み込む際は右辺ベクトルだけ更新してsolveすれば良い。
- なので、ソルバを EMField のメンバ変数としておき、initialize時にソルバを構築する設計にするのが良さそう。
- Dirichlet, Neumann, periodic 境界に対応。